{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import play\n",
    "from tdlearner import TDPlayer\n",
    "from tic_env import OptimalPlayer\n",
    "\n",
    "random_player = OptimalPlayer(epsilon=1.0)\n",
    "q_player = TDPlayer(epsilon=0.01)\n",
    "\n",
    "play(random_player, q_player, episodes=20000, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_games=500, loss=0.1034127026796341\n",
      "num_games=500, loss=0.0958690494298935\n",
      "num_games=500, loss=0.12122249603271484\n",
      "num_games=500, loss=0.17503491044044495\n",
      "num_games=1000, loss=0.1437593549489975\n",
      "num_games=1000, loss=0.16764579713344574\n",
      "num_games=1000, loss=0.12184719741344452\n",
      "num_games=1000, loss=0.13285285234451294\n",
      "num_games=1000, loss=0.14790916442871094\n",
      "num_games=1500, loss=0.14475387334823608\n",
      "num_games=1500, loss=0.10704521089792252\n",
      "num_games=1500, loss=0.20375925302505493\n",
      "num_games=1500, loss=0.15712682902812958\n",
      "num_games=1500, loss=0.11401726305484772\n",
      "num_games=2000, loss=0.1398276537656784\n",
      "num_games=2000, loss=0.1753837764263153\n",
      "num_games=2000, loss=0.1441870629787445\n",
      "num_games=2500, loss=0.13539201021194458\n",
      "num_games=2500, loss=0.14834940433502197\n",
      "num_games=2500, loss=0.15872159600257874\n",
      "num_games=2500, loss=0.09140487015247345\n",
      "num_games=2500, loss=0.1490848809480667\n",
      "num_games=3000, loss=0.1272190809249878\n",
      "num_games=3000, loss=0.12993785738945007\n",
      "num_games=3000, loss=0.12411379814147949\n",
      "num_games=3000, loss=0.1848863959312439\n",
      "num_games=3000, loss=0.1788477599620819\n",
      "num_games=3000, loss=0.18107856810092926\n",
      "num_games=3500, loss=0.1712580770254135\n",
      "num_games=3500, loss=0.17190267145633698\n",
      "num_games=3500, loss=0.15924739837646484\n",
      "num_games=3500, loss=0.11472246050834656\n",
      "num_games=3500, loss=0.15816476941108704\n",
      "num_games=4000, loss=0.17681652307510376\n",
      "num_games=4000, loss=0.17483863234519958\n",
      "num_games=4000, loss=0.17460723221302032\n",
      "num_games=4000, loss=0.16664917767047882\n",
      "num_games=4500, loss=0.14551055431365967\n",
      "num_games=4500, loss=0.201812282204628\n",
      "num_games=4500, loss=0.17162099480628967\n",
      "num_games=4500, loss=0.14136111736297607\n",
      "num_games=4500, loss=0.14609795808792114\n",
      "num_games=5000, loss=0.14036329090595245\n",
      "num_games=5000, loss=0.12629804015159607\n",
      "num_games=5000, loss=0.17337922751903534\n",
      "num_games=5000, loss=0.16726331412792206\n",
      "num_games=5500, loss=0.19736453890800476\n",
      "num_games=5500, loss=0.1503109335899353\n",
      "num_games=5500, loss=0.18684761226177216\n",
      "num_games=5500, loss=0.13185305893421173\n",
      "num_games=6000, loss=0.10982422530651093\n",
      "num_games=6000, loss=0.14430665969848633\n",
      "num_games=6000, loss=0.14154651761054993\n",
      "num_games=6500, loss=0.19001708924770355\n",
      "num_games=6500, loss=0.11923286318778992\n",
      "num_games=6500, loss=0.2012038230895996\n",
      "num_games=6500, loss=0.1601264327764511\n",
      "num_games=7000, loss=0.19313964247703552\n",
      "num_games=7000, loss=0.16420438885688782\n",
      "num_games=7000, loss=0.12048611789941788\n",
      "num_games=7000, loss=0.1524002104997635\n",
      "num_games=7000, loss=0.139810711145401\n",
      "num_games=7500, loss=0.15208208560943604\n",
      "num_games=7500, loss=0.15548153221607208\n",
      "num_games=7500, loss=0.20520927011966705\n",
      "num_games=7500, loss=0.1980838179588318\n",
      "num_games=8000, loss=0.14558403193950653\n",
      "num_games=8000, loss=0.14791026711463928\n",
      "num_games=8000, loss=0.13529685139656067\n",
      "num_games=8000, loss=0.12678402662277222\n",
      "num_games=8000, loss=0.142436221241951\n",
      "num_games=8500, loss=0.16424864530563354\n",
      "num_games=8500, loss=0.1406324803829193\n",
      "num_games=8500, loss=0.2109757661819458\n",
      "num_games=8500, loss=0.17185845971107483\n",
      "num_games=9000, loss=0.12296012043952942\n",
      "num_games=9000, loss=0.15508341789245605\n",
      "num_games=9000, loss=0.14131972193717957\n",
      "num_games=9500, loss=0.09888268262147903\n",
      "num_games=9500, loss=0.21653398871421814\n",
      "num_games=9500, loss=0.19272387027740479\n",
      "num_games=9500, loss=0.15994349122047424\n",
      "num_games=9500, loss=0.1692918986082077\n",
      "num_games=10000, loss=0.1306697428226471\n",
      "num_games=10000, loss=0.11594498157501221\n",
      "num_games=10000, loss=0.11383320391178131\n",
      "num_games=10000, loss=0.19896897673606873\n",
      "num_games=10500, loss=0.16291949152946472\n",
      "num_games=10500, loss=0.18700572848320007\n",
      "num_games=10500, loss=0.15038099884986877\n",
      "num_games=10500, loss=0.11661186814308167\n",
      "num_games=11000, loss=0.18076199293136597\n",
      "num_games=11000, loss=0.12922093272209167\n",
      "num_games=11000, loss=0.1344364583492279\n",
      "num_games=11000, loss=0.16535437107086182\n",
      "num_games=11000, loss=0.1373240351676941\n",
      "num_games=11500, loss=0.1590573787689209\n",
      "num_games=11500, loss=0.11143793910741806\n",
      "num_games=11500, loss=0.151154026389122\n",
      "num_games=11500, loss=0.12975749373435974\n",
      "num_games=12000, loss=0.1739746779203415\n",
      "num_games=12000, loss=0.14626207947731018\n",
      "num_games=12000, loss=0.10425528883934021\n",
      "num_games=12000, loss=0.17987719178199768\n",
      "num_games=12500, loss=0.1630232334136963\n",
      "num_games=12500, loss=0.1174967885017395\n",
      "num_games=12500, loss=0.19710063934326172\n",
      "num_games=12500, loss=0.1712256669998169\n",
      "num_games=13000, loss=0.17506510019302368\n",
      "num_games=13000, loss=0.09695828706026077\n",
      "num_games=13000, loss=0.14459490776062012\n",
      "num_games=13500, loss=0.1599602997303009\n",
      "num_games=13500, loss=0.16685304045677185\n",
      "num_games=13500, loss=0.1529667228460312\n",
      "num_games=13500, loss=0.16976210474967957\n",
      "num_games=14000, loss=0.15161877870559692\n",
      "num_games=14000, loss=0.1363823115825653\n",
      "num_games=14000, loss=0.1196078434586525\n",
      "num_games=14000, loss=0.10495597124099731\n",
      "num_games=14000, loss=0.1636866182088852\n",
      "num_games=14500, loss=0.16607066988945007\n",
      "num_games=14500, loss=0.12732163071632385\n",
      "num_games=14500, loss=0.14158830046653748\n",
      "num_games=14500, loss=0.15077072381973267\n",
      "num_games=15000, loss=0.16397856175899506\n",
      "num_games=15000, loss=0.13815370202064514\n",
      "num_games=15000, loss=0.17431804537773132\n",
      "num_games=15500, loss=0.1358056515455246\n",
      "num_games=15500, loss=0.15172669291496277\n",
      "num_games=15500, loss=0.1300438493490219\n",
      "num_games=15500, loss=0.14109309017658234\n",
      "num_games=15500, loss=0.15600264072418213\n",
      "num_games=16000, loss=0.13895006477832794\n",
      "num_games=16000, loss=0.19553618133068085\n",
      "num_games=16000, loss=0.1658487617969513\n",
      "num_games=16000, loss=0.18365710973739624\n",
      "num_games=16500, loss=0.1413794606924057\n",
      "num_games=16500, loss=0.15440309047698975\n",
      "num_games=16500, loss=0.1488761007785797\n",
      "num_games=16500, loss=0.18909716606140137\n",
      "num_games=16500, loss=0.1795758754014969\n",
      "num_games=17000, loss=0.15126602351665497\n",
      "num_games=17000, loss=0.1527939885854721\n",
      "num_games=17000, loss=0.12369777262210846\n",
      "num_games=17000, loss=0.1307900846004486\n",
      "num_games=17000, loss=0.1621071994304657\n",
      "num_games=17000, loss=0.15628311038017273\n",
      "num_games=17500, loss=0.16062788665294647\n",
      "num_games=17500, loss=0.19277098774909973\n",
      "num_games=17500, loss=0.14495578408241272\n",
      "num_games=17500, loss=0.14375180006027222\n",
      "num_games=17500, loss=0.15964114665985107\n",
      "num_games=18000, loss=0.15921762585639954\n",
      "num_games=18000, loss=0.21406376361846924\n",
      "num_games=18000, loss=0.14897280931472778\n",
      "num_games=18000, loss=0.15989895164966583\n",
      "num_games=18500, loss=0.17692650854587555\n",
      "num_games=18500, loss=0.17963168025016785\n",
      "num_games=18500, loss=0.16583144664764404\n",
      "num_games=18500, loss=0.160311758518219\n",
      "num_games=18500, loss=0.14115315675735474\n",
      "num_games=19000, loss=0.23702244460582733\n",
      "num_games=19000, loss=0.14518776535987854\n",
      "num_games=19000, loss=0.16558139026165009\n",
      "num_games=19000, loss=0.20809590816497803\n",
      "num_games=19500, loss=0.15663005411624908\n",
      "num_games=19500, loss=0.21379314363002777\n",
      "num_games=19500, loss=0.15534766018390656\n",
      "num_games=19500, loss=0.14206528663635254\n",
      "num_games=19500, loss=0.17975012958049774\n",
      "num_games=20000, loss=0.15599089860916138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'wins': 9111, 'losses': 10558, 'M': -0.07235},\n",
       " {'wins': 10558, 'losses': 9111, 'M': 0.07235})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import play\n",
    "from deepqlearner import DeepQPlayer\n",
    "from tic_env import OptimalPlayer\n",
    "\n",
    "random_player = OptimalPlayer(epsilon=1.0)\n",
    "q_player = DeepQPlayer(epsilon=0.01, target_update=500, batch_size=64)\n",
    "\n",
    "play(random_player, q_player, episodes=20000, debug=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5770e5300d53e6f64d47d7e32e89bf51c6f2fdc3215612dcda7f4da521533ca6"
  },
  "kernelspec": {
   "display_name": "ANN_env_2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
