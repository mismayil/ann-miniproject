{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import play\n",
    "from tdlearner import TDPlayer\n",
    "from tic_env import OptimalPlayer\n",
    "\n",
    "random_player = OptimalPlayer(epsilon=1.0)\n",
    "q_player = TDPlayer(epsilon=0.01)\n",
    "\n",
    "play(random_player, q_player, episodes=20000, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mismayil/Desktop/EPFL/W2022/ANN/ann-miniproject/deepqlearner.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.memory.append(Transition(state=torch.tensor(state),\n",
      "/Users/mismayil/Desktop/EPFL/W2022/ANN/ann-miniproject/deepqlearner.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state=torch.tensor(next_state) if next_state is not None else None,\n",
      "/Users/mismayil/opt/anaconda3/envs/annproject/lib/python3.8/site-packages/torch/nn/modules/loss.py:981: UserWarning: Using a target size (torch.Size([64, 1, 64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_games=500, loss=0.147556334733963\n",
      "num_games=500, loss=0.15135550498962402\n",
      "num_games=500, loss=0.19541756808757782\n",
      "num_games=1000, loss=0.1475965827703476\n",
      "num_games=1000, loss=0.14182719588279724\n",
      "num_games=1000, loss=0.16059863567352295\n",
      "num_games=1500, loss=0.1318572461605072\n",
      "num_games=1500, loss=0.17981356382369995\n",
      "num_games=1500, loss=0.1595131903886795\n",
      "num_games=2000, loss=0.17843395471572876\n",
      "num_games=2000, loss=0.18424002826213837\n",
      "num_games=2000, loss=0.2033759355545044\n",
      "num_games=2000, loss=0.20945656299591064\n",
      "num_games=2500, loss=0.2144092321395874\n",
      "num_games=2500, loss=0.20564110577106476\n",
      "num_games=2500, loss=0.1858583688735962\n",
      "num_games=2500, loss=0.2046937197446823\n",
      "num_games=3000, loss=0.16696423292160034\n",
      "num_games=3000, loss=0.24624022841453552\n",
      "num_games=3000, loss=0.21909379959106445\n",
      "num_games=3500, loss=0.23054607212543488\n",
      "num_games=3500, loss=0.19797641038894653\n",
      "num_games=3500, loss=0.21453377604484558\n",
      "num_games=4000, loss=0.18760257959365845\n",
      "num_games=4000, loss=0.21367216110229492\n",
      "num_games=4000, loss=0.17667625844478607\n",
      "num_games=4500, loss=0.21786215901374817\n",
      "num_games=4500, loss=0.2438156008720398\n",
      "num_games=4500, loss=0.23134227097034454\n",
      "num_games=4500, loss=0.19770114123821259\n",
      "num_games=5000, loss=0.22760213911533356\n",
      "num_games=5000, loss=0.22048884630203247\n",
      "num_games=5500, loss=0.2123100459575653\n",
      "num_games=5500, loss=0.22003917396068573\n",
      "num_games=5500, loss=0.18301036953926086\n",
      "num_games=6000, loss=0.21780437231063843\n",
      "num_games=6000, loss=0.21348032355308533\n",
      "num_games=6000, loss=0.2301945835351944\n",
      "num_games=6000, loss=0.2418479472398758\n",
      "num_games=6500, loss=0.2129751741886139\n",
      "num_games=6500, loss=0.2131367027759552\n",
      "num_games=6500, loss=0.20011964440345764\n",
      "num_games=7000, loss=0.2084100842475891\n",
      "num_games=7000, loss=0.212281733751297\n",
      "num_games=7000, loss=0.24124786257743835\n",
      "num_games=7500, loss=0.23107698559761047\n",
      "num_games=7500, loss=0.23332688212394714\n",
      "num_games=7500, loss=0.2510339617729187\n",
      "num_games=8000, loss=0.2278459668159485\n",
      "num_games=8000, loss=0.23464226722717285\n",
      "num_games=8000, loss=0.2252037525177002\n",
      "num_games=8000, loss=0.20524665713310242\n",
      "num_games=8500, loss=0.23273159563541412\n",
      "num_games=8500, loss=0.22236251831054688\n",
      "num_games=8500, loss=0.22485804557800293\n",
      "num_games=9000, loss=0.19957415759563446\n",
      "num_games=9000, loss=0.21955209970474243\n",
      "num_games=9000, loss=0.2091025412082672\n",
      "num_games=9500, loss=0.24251237511634827\n",
      "num_games=9500, loss=0.2253388613462448\n",
      "num_games=9500, loss=0.23508423566818237\n",
      "num_games=9500, loss=0.2261984795331955\n",
      "num_games=10000, loss=0.21608659625053406\n",
      "num_games=10000, loss=0.22235938906669617\n",
      "num_games=10000, loss=0.23921795189380646\n",
      "num_games=10500, loss=0.23237034678459167\n",
      "num_games=10500, loss=0.17892654240131378\n",
      "num_games=10500, loss=0.22858168184757233\n",
      "num_games=11000, loss=0.23849326372146606\n",
      "num_games=11000, loss=0.22685953974723816\n",
      "num_games=11000, loss=0.22650739550590515\n",
      "num_games=11000, loss=0.23465391993522644\n",
      "num_games=11500, loss=0.20422112941741943\n",
      "num_games=11500, loss=0.20962199568748474\n",
      "num_games=12000, loss=0.2530762255191803\n",
      "num_games=12000, loss=0.23228071630001068\n",
      "num_games=12000, loss=0.22731685638427734\n",
      "num_games=12000, loss=0.18034441769123077\n",
      "num_games=12500, loss=0.23071527481079102\n",
      "num_games=12500, loss=0.24329829216003418\n",
      "num_games=12500, loss=0.1806049644947052\n",
      "num_games=12500, loss=0.17694710195064545\n",
      "num_games=13000, loss=0.22771281003952026\n",
      "num_games=13000, loss=0.22732119262218475\n",
      "num_games=13500, loss=0.22780472040176392\n",
      "num_games=13500, loss=0.20117715001106262\n",
      "num_games=13500, loss=0.22157201170921326\n",
      "num_games=14000, loss=0.23662695288658142\n",
      "num_games=14000, loss=0.1989668309688568\n",
      "num_games=14000, loss=0.21793600916862488\n",
      "num_games=14500, loss=0.21260899305343628\n",
      "num_games=14500, loss=0.23393666744232178\n",
      "num_games=14500, loss=0.1970452070236206\n",
      "num_games=14500, loss=0.2225848287343979\n",
      "num_games=15000, loss=0.2224087417125702\n",
      "num_games=15000, loss=0.239406555891037\n",
      "num_games=15000, loss=0.22352932393550873\n",
      "num_games=15000, loss=0.23328764736652374\n",
      "num_games=15000, loss=0.21933387219905853\n",
      "num_games=15500, loss=0.2195742428302765\n",
      "num_games=15500, loss=0.19975578784942627\n",
      "num_games=15500, loss=0.21989664435386658\n",
      "num_games=16000, loss=0.24025067687034607\n",
      "num_games=16000, loss=0.2625419497489929\n",
      "num_games=16000, loss=0.21493977308273315\n",
      "num_games=16000, loss=0.2480333000421524\n",
      "num_games=16500, loss=0.24763000011444092\n",
      "num_games=16500, loss=0.2230241745710373\n",
      "num_games=16500, loss=0.2247060239315033\n",
      "num_games=17000, loss=0.18997645378112793\n",
      "num_games=17000, loss=0.20545771718025208\n",
      "num_games=17000, loss=0.2339511215686798\n",
      "num_games=17000, loss=0.2062157243490219\n",
      "num_games=17000, loss=0.23293718695640564\n",
      "num_games=17000, loss=0.20858372747898102\n",
      "num_games=17500, loss=0.19909106194972992\n",
      "num_games=17500, loss=0.22078904509544373\n",
      "num_games=17500, loss=0.20977719128131866\n",
      "num_games=18000, loss=0.22374998033046722\n",
      "num_games=18000, loss=0.1994876265525818\n",
      "num_games=18000, loss=0.20965084433555603\n",
      "num_games=18500, loss=0.22114579379558563\n",
      "num_games=18500, loss=0.17508941888809204\n",
      "num_games=18500, loss=0.24260181188583374\n",
      "num_games=18500, loss=0.22325170040130615\n",
      "num_games=18500, loss=0.2416554093360901\n",
      "num_games=19000, loss=0.21823926270008087\n",
      "num_games=19000, loss=0.2181127816438675\n",
      "num_games=19000, loss=0.2325901836156845\n",
      "num_games=19500, loss=0.2075674831867218\n",
      "num_games=19500, loss=0.2352491021156311\n",
      "num_games=19500, loss=0.22718445956707\n",
      "num_games=19500, loss=0.21586185693740845\n",
      "num_games=20000, loss=0.1990671157836914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'wins': 18881, 'losses': 1070, 'M': 0.89055},\n",
       " {'wins': 1070, 'losses': 18881, 'M': -0.89055})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import play\n",
    "from deepqlearner import DeepQPlayer\n",
    "from tic_env import OptimalPlayer\n",
    "\n",
    "random_player = OptimalPlayer(epsilon=0.5)\n",
    "q_player = DeepQPlayer(epsilon=0.01, target_update=500, batch_size=64)\n",
    "\n",
    "play(random_player, q_player, episodes=20000, debug=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5770e5300d53e6f64d47d7e32e89bf51c6f2fdc3215612dcda7f4da521533ca6"
  },
  "kernelspec": {
   "display_name": "ANN_env_2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
