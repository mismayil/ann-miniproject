{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1: Tic Tac Toe\n",
    "This notebook contains the code for Miniproject 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "Here we define some utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "\n",
    "def play(player1, player2, episodes=5, debug=False, first_player=\"alternate\", disable_tqdm=False, seed=None):\n",
    "    \"\"\"Play Tic Tact Toe between two players\n",
    "\n",
    "    Args:\n",
    "        player1: Player 1\n",
    "        player2: Player 2\n",
    "        episodes (int, optional): Number of episodes. Defaults to 5.\n",
    "        debug (bool, optional): Whether to print debug messages. Defaults to False.\n",
    "        first_player (str, optional): Strategy to determine first player. Defaults to \"alternate\".\n",
    "            \"alternate\" means alternate between player1 and player2 every game. Otherwise, randomly determined.\n",
    "        disable_tqdm (bool, optional): Whether to disable progress bar. Defaults to False.\n",
    "        seed (_type_, optional): Random seed. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        (dict, dict): Player 1 stats and Player 2 stats (M-values)\n",
    "    \"\"\"\n",
    "    env = TictactoeEnv()\n",
    "    Turns = np.array(['X','O'])\n",
    "    player1_stats = {'wins': 0, 'losses': 0, 'M': 0}\n",
    "    player2_stats = {'wins': 0, 'losses': 0, 'M': 0}\n",
    "    \n",
    "    if seed is not None: \n",
    "        # Set a seed for reproducibility\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    for i in tqdm(range(episodes), disable=disable_tqdm):\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        invalid_player = None\n",
    "\n",
    "        if first_player == \"alternate\":\n",
    "            Turns = np.flip(Turns)\n",
    "        else:\n",
    "            Turns = Turns[np.random.permutation(2)]\n",
    "\n",
    "        player1.set_player(Turns[0])\n",
    "        player2.set_player(Turns[1])\n",
    "\n",
    "        if debug:\n",
    "            print('-------------------------------------------')\n",
    "            print(f\"Game {i}, Player 1 = {Turns[0]}, Player 2 = {Turns[1]}\")\n",
    "\n",
    "        for j in range(9):\n",
    "            if env.current_player == player1.player:\n",
    "                move = player1.act(grid)\n",
    "            else:\n",
    "                move = player2.act(grid)\n",
    "\n",
    "            try:\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "            except ValueError:\n",
    "                # If wrong move is played, penalize the player\n",
    "                end = True\n",
    "                invalid_player = player1.player if env.current_player == player1.player else player2.player\n",
    "                winner = None\n",
    "\n",
    "            if end:\n",
    "                if hasattr(player1, 'end'):\n",
    "                    player1.end(grid, winner, invalid_move=(invalid_player==player1.player))\n",
    "                \n",
    "                if hasattr(player2, 'end'):\n",
    "                    player2.end(grid, winner, invalid_move=(invalid_player==player2.player))\n",
    "\n",
    "                if winner == player1.player:\n",
    "                    player1_stats['wins'] += 1\n",
    "                    player2_stats['losses'] += 1\n",
    "                elif winner == player2.player:\n",
    "                    player1_stats['losses'] += 1\n",
    "                    player2_stats['wins'] += 1\n",
    "                \n",
    "                if debug:\n",
    "                    print('-------------------------------------------')\n",
    "                    print('Game end, winner is player ' + str(winner))\n",
    "                    print('Player 1 = ' +  Turns[0])\n",
    "                    print('Player 2 = ' +  Turns[1])\n",
    "                    env.render()\n",
    "                    print('-------------------------------------------')\n",
    "                \n",
    "                break\n",
    "    \n",
    "    player1_stats['M'] = (player1_stats['wins'] - player1_stats['losses']) / episodes\n",
    "    player2_stats['M'] = (player2_stats['wins'] - player2_stats['losses']) / episodes\n",
    "\n",
    "    if hasattr(player1, 'finish_run'):\n",
    "        player1.finish_run()\n",
    "\n",
    "    if hasattr(player2, 'finish_run'):\n",
    "        player2.finish_run()\n",
    "\n",
    "    return player1_stats, player2_stats\n",
    "\n",
    "\n",
    "def calculate_m_opt(q_player, episodes=500):\n",
    "    \"\"\"Calculate M_opt for a given player\n",
    "\n",
    "    Args:\n",
    "        q_player: Player\n",
    "        episodes (int, optional): Number of episodes. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        float: M_opt value\n",
    "    \"\"\"\n",
    "    # Put player in evaluation mode to avoid training and logging\n",
    "    if hasattr(q_player, 'eval'): \n",
    "        q_player.eval()\n",
    "\n",
    "    optimal_player = OptimalPlayer(epsilon=0.0)\n",
    "    player1_stats, _ = play(q_player, optimal_player, episodes=episodes, debug=False, first_player='alternate', disable_tqdm=True)\n",
    "\n",
    "    # Put player back in training mode\n",
    "    if hasattr(q_player, 'train'): \n",
    "        q_player.train()\n",
    "\n",
    "    return player1_stats['M']\n",
    "\n",
    "def calculate_m_rand(q_player, episodes=500):\n",
    "    \"\"\"Calculate M_rand for a given player\n",
    "\n",
    "    Args:\n",
    "        q_player: Player\n",
    "        episodes (int, optional): Number of episodes. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        float: M_rand value\n",
    "    \"\"\"\n",
    "    # Put player in evaluation mode to avoid training and logging\n",
    "    if hasattr(q_player, 'eval'): \n",
    "        q_player.eval()\n",
    "\n",
    "    random_player = OptimalPlayer(epsilon=1.0)\n",
    "    player1_stats, _ = play(q_player, random_player, episodes=episodes, debug=False, first_player='alternate', disable_tqdm=True)\n",
    "\n",
    "    # Put player back in training mode\n",
    "    if hasattr(q_player, 'train'): \n",
    "        q_player.train()\n",
    "\n",
    "    return player1_stats['M']\n",
    "\n",
    "def save_stats(players, path):\n",
    "    \"\"\" Save player stats to a file\"\"\"\n",
    "    player_stats = []\n",
    "    \n",
    "    for player in players:\n",
    "        \n",
    "        stat = {\n",
    "            'loss': player.avg_losses if hasattr(player, 'avg_losses') else None,\n",
    "            'reward': player.avg_rewards,\n",
    "            'm_opt': player.m_values['m_opt'],\n",
    "            'm_rand': player.m_values['m_rand']\n",
    "        }\n",
    "        player_stats.append(stat)\n",
    "        \n",
    "    with open(path, 'wb') as npy:\n",
    "        np.save(npy, player_stats)\n",
    "\n",
    "def plot_average_rewards(stats_path, labels, log_every=250, save_path=None):\n",
    "    try:\n",
    "        with open(stats_path, 'rb') as npy:\n",
    "            player_stats = np.load(npy, allow_pickle=True)\n",
    "    except:\n",
    "        print('File not found!')\n",
    "        raise\n",
    "        \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "    game_ids = list(range(0, len(player_stats[0]['reward'])*log_every, log_every))\n",
    "    for i, player in enumerate(player_stats):\n",
    "        label = labels[i]\n",
    "        ax.plot(game_ids, player['reward'], label=label)\n",
    "    \n",
    "    ax.set_title(f'Average reward per {log_every} games', fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel('Game', fontsize=16)\n",
    "    ax.set_ylabel('Reward', fontsize=16)\n",
    "    ax.set_xlim([0, len(game_ids)*log_every])\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    if save_path is not None: fig.savefig(save_path, format='pdf')\n",
    "\n",
    "def plot_m_values(stats_path, labels, test_every=250, save_path=None):\n",
    "    try:\n",
    "        with open(stats_path, 'rb') as npy:\n",
    "            player_stats = np.load(npy, allow_pickle=True)\n",
    "    except:\n",
    "        print('File not found!')\n",
    "        raise\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    game_ids = list(range(0, len(player_stats[0]['reward'])*test_every, test_every))\n",
    "    for i, player in enumerate(player_stats):\n",
    "        label = labels[i]\n",
    "        axes[0].plot(game_ids, player[\"m_opt\"], label=label)\n",
    "        axes[1].plot(game_ids, player[\"m_rand\"], label=label)\n",
    "\n",
    "        \n",
    "    axes[0].set_title(f'M_opt per {test_every} games', fontsize=20, fontweight='bold')\n",
    "    axes[0].set_xlabel('Game', fontsize=16)\n",
    "    axes[0].set_ylabel('M_opt', fontsize=16)\n",
    "    axes[0].set_xlim([0, len(game_ids)*test_every])\n",
    "    axes[0].legend()\n",
    "    axes[0].grid()\n",
    "    \n",
    "    axes[1].set_title(f'M_rand per {test_every} games', fontsize=20, fontweight='bold')\n",
    "    axes[1].set_xlabel('Game', fontsize=16)\n",
    "    axes[1].set_ylabel('M_rand', fontsize=16)\n",
    "    axes[1].set_xlim([0, len(game_ids)*test_every])\n",
    "    axes[1].legend()\n",
    "    axes[1].grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    if save_path is not None: fig.savefig(save_path, format='pdf')\n",
    "\n",
    "def read_grid(grid):\n",
    "    empty = list(zip(*np.where(grid==0)))\n",
    "    filled = list(zip(*np.where(grid!=0)))\n",
    "    return empty, filled\n",
    "\n",
    "def plot_heatmaps(states_list, qvalues_list, titles, cmap=None, save_path=None):\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    axes = AxesGrid(fig, 111,\n",
    "                nrows_ncols=(1, 3),\n",
    "                axes_pad=0.2,\n",
    "                cbar_mode='single',\n",
    "                cbar_location='right',\n",
    "                cbar_pad=0.1\n",
    "                )\n",
    "    \n",
    "    if cmap is None: cmap = plt.cm.get_cmap('Blues', 10)\n",
    "    \n",
    "    for ax, state, qvalues, title in zip(axes, states_list, qvalues_list, titles):\n",
    "        \n",
    "        empty, filled = read_grid(state) # Read the configuration of the grid\n",
    "        qvalue_grid = -1*np.ones((3,3))\n",
    "\n",
    "        for action in empty:\n",
    "            qstate = QStateAction(state, action)\n",
    "            qvalue_grid[action] = qvalues.get(qstate, 0)\n",
    "        \n",
    "\n",
    "        img = ax.imshow(qvalue_grid, cmap=cmap, vmin=-1, vmax=1)\n",
    "        ax.set_axis_off()\n",
    "\n",
    "        for i, j in empty:\n",
    "            qval = qvalue_grid[i, j]\n",
    "            text = ax.text(j, i, f'{qval:.3f}', ha='center', va='center', color='k', fontsize=14)\n",
    "\n",
    "        for i, j in filled:\n",
    "            player = 'X' if state[i, j] == 1 else 'O'\n",
    "            text = ax.text(j, i, f'{player}', ha='center', va='center', color='k', fontsize=16, fontweight='bold')\n",
    "            \n",
    "        \n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            \n",
    "            \n",
    "    cbar = ax.cax.colorbar(img)\n",
    "    cbar = axes.cbar_axes[0].colorbar(img)\n",
    "\n",
    "\n",
    "    if save_path is not None: fig.savefig(save_path, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QStateAction:\n",
    "    \"\"\"\n",
    "    This is a helper class to store the state-action pair.\n",
    "    \"\"\"\n",
    "    def __init__(self, grid, action):\n",
    "        self.grid = grid\n",
    "        self.state = tuple(grid.ravel().tolist())\n",
    "        self.action = action\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash((self.state, self.action))\n",
    "    \n",
    "    def __eq__(self, other) -> bool:\n",
    "        return isinstance(other, QStateAction) and (self.state == other.state) and self.action == other.action\n",
    "\n",
    "    def __ne__(self, other) -> bool:\n",
    "        return not self.__eq__(other)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"state={self.state}, action={self.action}\"\n",
    "\n",
    "\n",
    "class QPlayer:\n",
    "    def __init__(self, epsilon=0.01, alpha=0.05, gamma=0.99, player='X', log_every=250, test_every=None, qvalues=None, wandb_name=None, *args, **kwargs):\n",
    "        \"\"\"Initialize a Q-learning player\n",
    "\n",
    "        Args:\n",
    "            epsilon (float, optional): Epsilon value. Defaults to 0.01.\n",
    "            alpha (float, optional): Alpha value. Defaults to 0.05.\n",
    "            gamma (float, optional): Gamma value. Defaults to 0.99.\n",
    "            player (str, optional): Player symbol. Defaults to 'X'.\n",
    "            log_every (int, optional): Logging frequency (i.e. for logging avg reward). Defaults to 250.\n",
    "            test_every (int, optional): Testing frequency (i.e. for logging M_opt and M_rand). Defaults to None.\n",
    "            qvalues (dict, optional): Q-values dict. Defaults to None.\n",
    "            wandb_name (str, optional): Wandb run name. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.player = player # 'X' or 'O'\n",
    "        self.qvalues = defaultdict(int) if qvalues is None else qvalues\n",
    "        self.last_qstate = None\n",
    "        self.last_reward = 0\n",
    "        self.num_games = 0\n",
    "        self.running_reward = 0\n",
    "        self.avg_rewards = []\n",
    "        self.m_values = {\"m_opt\": [], \"m_rand\": []}\n",
    "        self.log_every = log_every\n",
    "        self.log = False if log_every is None or log_every <=0 else True\n",
    "        self.test_every = test_every\n",
    "        self.test = False if test_every is None or log_every <=0 else True\n",
    "        self.eval_mode = False\n",
    "        self.wandb_name = wandb_name\n",
    "        self.wandb_run = None\n",
    "\n",
    "        if self.log and self.wandb_name is not None:\n",
    "            import wandb\n",
    "            self.wandb_run = wandb.init(project=\"ann-project\", name=wandb_name, reinit=True,\n",
    "                                        config={\"epsilon\": epsilon, \"gamma\": gamma, \"player\": player,\n",
    "                                                \"log_every\": log_every, \"test_every\": test_every, \"alpha\": alpha})\n",
    "        \n",
    "        \n",
    "    def set_player(self, player = 'X', j=-1):\n",
    "        self.player = player\n",
    "        self.last_qstate = None\n",
    "        self.last_reward = 0\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "            \n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Put player in evaluation mode. In this mode, player does not train\n",
    "        and does not log metrics.\n",
    "        \"\"\"\n",
    "        self.eval_mode = True\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Put player in training mode. In this mode, player trains and logs metrics.\n",
    "        \"\"\"\n",
    "        self.eval_mode = False\n",
    "\n",
    "    def finish_run(self):\n",
    "        \"\"\"Finish wandb run if present and if in training mode\"\"\"\n",
    "        if not self.eval_mode and self.wandb_run:\n",
    "            self.wandb_run.finish()\n",
    "\n",
    "    def random(self, grid):\n",
    "        \"\"\"Chose a random action from the available options. \"\"\"\n",
    "        avail = self.empty(grid)\n",
    "        return QStateAction(grid, avail[random.randint(0, len(avail)-1)])\n",
    "\n",
    "    def empty(self, grid):\n",
    "        \"\"\"Return all empty cells from the grid.\"\"\"\n",
    "        avail = np.where(grid==0)\n",
    "        return list(zip(*avail))\n",
    "\n",
    "    def greedy(self, grid):\n",
    "        \"\"\"Return the best action according to the current Q-values.\"\"\"\n",
    "        actions = self.empty(grid)\n",
    "        best_qstates = None\n",
    "        max_qvalue = None\n",
    "\n",
    "        for action in actions:\n",
    "            qstate = QStateAction(grid, action)\n",
    "            qvalue = self.qvalues.get(qstate, 0)\n",
    "            \n",
    "            if max_qvalue is None or qvalue > max_qvalue:\n",
    "                max_qvalue = qvalue\n",
    "                best_qstates = [qstate]\n",
    "            elif max_qvalue is not None and qvalue==max_qvalue:\n",
    "                best_qstates.append(qstate)\n",
    "        \n",
    "        return np.random.choice(best_qstates)\n",
    "\n",
    "    def opponent(self):\n",
    "        \"\"\"Get the opponent player symbol.\"\"\"\n",
    "        return 'X' if self.player == 'O' else 'O'\n",
    "\n",
    "    def decide(self, grid):\n",
    "        \"\"\"Decide on the next action.\"\"\"\n",
    "        epsilon = self.epsilon(self.num_games) if callable(self.epsilon) else self.epsilon\n",
    "        if self.eval_mode or random.random() > epsilon:\n",
    "            return self.greedy(grid)\n",
    "        return self.random(grid)\n",
    "\n",
    "    def update(self, grid, reward=0, end=False):\n",
    "        \"\"\"Update the Q-values based on the last action.\"\"\"\n",
    "        next_value = 0\n",
    "\n",
    "        if not end:\n",
    "            next_value = self.qvalues[self.greedy(grid)]\n",
    "\n",
    "        if self.last_qstate:\n",
    "            self.qvalues[self.last_qstate] += self.alpha * (reward + self.gamma * next_value - self.qvalues[self.last_qstate])\n",
    "\n",
    "    def end(self, grid, winner, *args, **kwargs):\n",
    "        \"\"\"End of game callback. Update the Q-values based on the last action and log metrics\"\"\"\n",
    "        if self.eval_mode:\n",
    "            return\n",
    "        \n",
    "        self.num_games += 1\n",
    "        reward = 0\n",
    "\n",
    "        if winner == self.player:\n",
    "            reward = 1\n",
    "        elif winner == self.opponent():\n",
    "            reward = -1\n",
    "\n",
    "        self.update(grid, reward=reward, end=True)\n",
    "\n",
    "        self.last_qstate = None\n",
    "        \n",
    "        if self.log:\n",
    "            self.running_reward += reward\n",
    "            \n",
    "            if (self.num_games+1) % self.log_every == 0:\n",
    "                avg_reward = self.running_reward / self.log_every\n",
    "                self.avg_rewards.append(avg_reward)\n",
    "                self.running_reward = 0\n",
    "\n",
    "                if self.wandb_name is not None:\n",
    "                    import wandb\n",
    "                    wandb.log({\"avg_reward\": avg_reward})\n",
    "                \n",
    "        if self.test:\n",
    "            if  (self.num_games+1) % self.test_every == 0:\n",
    "                m_opt = calculate_m_opt(self)\n",
    "                m_rand = calculate_m_rand(self)\n",
    "\n",
    "                self.m_values[\"m_opt\"].append(m_opt)\n",
    "                self.m_values[\"m_rand\"].append(m_rand)\n",
    "\n",
    "                if self.wandb_name is not None:\n",
    "                    import wandb\n",
    "                    wandb.log({\"m_opt\": m_opt, \"m_rand\": m_rand})\n",
    "                \n",
    "\n",
    "    def act(self, grid):\n",
    "        \"\"\"Act on the grid.\"\"\"\n",
    "        qstate = self.decide(grid)\n",
    "\n",
    "        if not self.eval_mode: \n",
    "            self.update(grid)\n",
    "\n",
    "        self.last_qstate = qstate\n",
    "        return qstate.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tic_env import OptimalPlayer\n",
    "\n",
    "epsilons = [0.01, 0.1, 0.2, 0.5, 0.75]\n",
    "eps_q_players = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    suboptimal_player = OptimalPlayer(epsilon=0.5)\n",
    "    q_player = QPlayer(epsilon=eps)\n",
    "    play(suboptimal_player, q_player, episodes=20000)\n",
    "    eps_q_players.append(q_player)\n",
    "\n",
    "save_stats(players=eps_q_players, path='answers/Q1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f'$\\epsilon$={eps}' for eps in epsilons]\n",
    "plot_average_rewards('answers/Q1.npy', labels=labels, save_path='artifacts/fig_Q1.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_MIN = 0.1\n",
    "EPS_MAX = 0.8\n",
    "n_stars = [1, 100, 1000, 10000, 40000]\n",
    "n_star_players = []\n",
    "\n",
    "\n",
    "for n_star in n_stars:\n",
    "    get_epsilon = lambda n, n_star=n_star: max(EPS_MIN, EPS_MAX * (1 - n / n_star))\n",
    "    suboptimal_player = OptimalPlayer(epsilon=0.5)\n",
    "    q_player = QPlayer(epsilon=get_epsilon)\n",
    "    play(suboptimal_player, q_player, episodes=20000)\n",
    "    n_star_players.append(q_player)\n",
    "    \n",
    "save_stats(players=n_star_players, path='answers/Q2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f'n*={n_star}' for n_star in n_stars]\n",
    "plot_average_rewards(stats_path='answers/Q2.npy', labels=labels, save_path='artifacts/fig_Q2.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_MIN = 0.1\n",
    "EPS_MAX = 0.8\n",
    "n_stars = [1, 100, 1000, 10000, 40000]\n",
    "n_star_players = []\n",
    "\n",
    "for n_star in n_stars:\n",
    "    get_epsilon = lambda n, n_star=n_star: max(EPS_MIN, EPS_MAX * (1 - n / n_star))\n",
    "    suboptimal_player = OptimalPlayer(epsilon=0.5)\n",
    "    q_player = QPlayer(epsilon=get_epsilon, test_every=250)\n",
    "    play(suboptimal_player, q_player, episodes=20000)\n",
    "    n_star_players.append(q_player)\n",
    "    \n",
    "save_stats(players=n_star_players, path='answers/Q3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f\"n*={n_star}\" for n_star in n_stars]\n",
    "plot_m_values(stats_path='answers/Q3.npy', labels=labels, save_path='artifacts/fig_Q3.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STAR = 100\n",
    "EPS_MIN = 0.1\n",
    "EPS_MAX = 0.8\n",
    "get_epsilon = lambda n: max(EPS_MIN, EPS_MAX * (1 - n / N_STAR))\n",
    "\n",
    "epsilons = [0, 0.01, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "eps_players = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    other_player = OptimalPlayer(epsilon=eps)\n",
    "    q_player = QPlayer(epsilon=get_epsilon, test_every=250)\n",
    "    play(other_player, q_player, episodes=20000)\n",
    "    eps_players.append(q_player)\n",
    "    \n",
    "save_stats(players=eps_players, path='answers/Q4.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f'$\\epsilon$={eps}' for eps in epsilons]\n",
    "plot_m_values(stats_path='answers/Q4.npy', labels=labels, save_path='artifacts/fig_Q4.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0.01, 0.1, 0.2, 0.5]\n",
    "self_q_players = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    qvalues = defaultdict(int)\n",
    "    q_player1 = QPlayer(epsilon=eps, qvalues=qvalues, test_every=250)\n",
    "    q_player2 = QPlayer(epsilon=eps, qvalues=qvalues, test_every=250)\n",
    "    play(q_player1, q_player2, episodes=20000)\n",
    "    self_q_players.append(q_player1)\n",
    "\n",
    "save_stats(players=self_q_players, path='answers/Q7.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f'$\\epsilon$={eps}' for eps in epsilons]\n",
    "plot_m_values(stats_path='answers/Q7.npy', labels=labels, save_path='artifacts/fig_Q7.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_MIN = 0.1\n",
    "EPS_MAX = 0.8\n",
    "n_stars = [1, 100, 1000, 10000, 40000]\n",
    "self_n_star_players = []\n",
    "\n",
    "for n_star in n_stars:\n",
    "    get_epsilon = lambda n, n_star=n_star: max(EPS_MIN, EPS_MAX * (1 - n / n_star))\n",
    "    qvalues = defaultdict(int)\n",
    "    q_player1 = QPlayer(epsilon=get_epsilon, qvalues=qvalues, test_every=250)\n",
    "    q_player2 = QPlayer(epsilon=get_epsilon, qvalues=qvalues, test_every=250)\n",
    "    play(q_player1, q_player2, episodes=20000)\n",
    "    self_n_star_players.append(q_player1)\n",
    "    \n",
    "save_stats(players=self_n_star_players, path='answers/Q8.npy')\n",
    "\n",
    "qvalues = [player.qvalues for player in self_n_star_players]\n",
    "with open('answers/Q10.npy', 'wb') as npy:\n",
    "    np.save(npy, qvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f'n*={n_star}' for n_star in n_stars]\n",
    "plot_m_values(stats_path='answers/Q8.npy', labels=labels, save_path='artifacts/fig_Q8.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = np.array([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, -1.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "])\n",
    "\n",
    "state2 = np.array([\n",
    "    [-1.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [-1.0, 0.0, 0.0],\n",
    "])\n",
    "\n",
    "state3 = np.array([\n",
    "    [0.0, 0.0, -1.0],\n",
    "    [0.0, 1.0, -1.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "])\n",
    "\n",
    "with open('answers/Q10.npy', 'rb') as npy:\n",
    "    qvalues = np.load(npy, allow_pickle=True)\n",
    "\n",
    "states_list = [state1, state2, state3]\n",
    "qvalues_list = [qvalues[4], qvalues[3], qvalues[2]]\n",
    "titles=[f'Self-Play with n*={n_star}' for n_star in [40000, 10000, 1000]]\n",
    "cmap = plt.cm.get_cmap('Blues', 100)\n",
    "\n",
    "plot_heatmaps(states_list, qvalues_list, titles=titles, cmap=cmap, save_path='artifacts/fig_Q10.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "VALUE_TO_PLAYER = {-1: 'O', 1: 'X', 0: None}\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, next_state=None, reward=0):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(state=state.clone(),\n",
    "                                      action=action.clone() if isinstance(action, torch.Tensor) else torch.tensor(action),\n",
    "                                      next_state=next_state.clone() if next_state is not None else None,\n",
    "                                      reward=reward.clone() if isinstance(reward, torch.Tensor) else torch.tensor(reward)))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\" Sample a batch of transitions \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, in_dim=18, out_dim=9, hidden_dim=128) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x.flatten(start_dim=1))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        predictions = self.forward(x)\n",
    "        return torch.argmax(predictions)\n",
    "\n",
    "\n",
    "class DeepQPlayer:\n",
    "    def __init__(self, epsilon=0.1, gamma=0.99, player='X', memory_capacity=10000, target_update=500,\n",
    "                 batch_size=64, learning_rate=5e-4, log_every=250, debug=False,\n",
    "                 policy_net=None, target_net=None, memory=None, swap_state=False, log=True, wandb_name=None, do_optimize=True, *args, **kwargs):\n",
    "        \"\"\"Initialize a Deep Q-learning player\n",
    "\n",
    "        Args:\n",
    "            epsilon (float, optional): Epsilon value. Defaults to 0.1.\n",
    "            gamma (float, optional): Gamma value. Defaults to 0.99.\n",
    "            player (str, optional): Player symbol. Defaults to 'X'.\n",
    "            memory_capacity (int, optional): Replay memory capacity. Defaults to 10000.\n",
    "            target_update (int, optional): Update frequency for target network. Defaults to 500.\n",
    "            batch_size (int, optional): Batch size. Defaults to 64.\n",
    "            learning_rate (float, optional): Learning rate. Defaults to 5e-4.\n",
    "            log_every (int, optional): Logging frequency. Defaults to 250.\n",
    "            debug (bool, optional): Whether print debug messages. Defaults to False.\n",
    "            policy_net (nn.Module, optional): Shared policy network. Defaults to None.\n",
    "            target_net (nn.Module, optional): Shared target network. Defaults to None.\n",
    "            memory (ReplayMemory, optional): Shared replay memory. Defaults to None.\n",
    "            swap_state (bool, optional): Whether to swap state before saving to memory. Defaults to False.\n",
    "            log (bool, optional): Whether to log metrics. Defaults to True.\n",
    "            wandb_name (str, optional): Wandb run name. Defaults to None.\n",
    "            do_optimize (bool, optional): Whether to optimize the policy network. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.player = player\n",
    "        self.memory = ReplayMemory(memory_capacity) if memory is None else memory\n",
    "        self.policy_net = DeepQNetwork().to(DEVICE) if policy_net is None else policy_net.to(DEVICE)\n",
    "        self.target_net = DeepQNetwork().to(DEVICE) if target_net is None else target_net.to(DEVICE)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.last_reward = 0\n",
    "        self.num_games = 0\n",
    "        self.target_update = target_update\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.HuberLoss()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        self.running_reward = 0\n",
    "        self.running_loss = 0\n",
    "        self.avg_rewards = []\n",
    "        self.avg_losses = []\n",
    "        self.log_every = log_every\n",
    "        self.debug = debug\n",
    "        self.m_values = {\"m_opt\": [], \"m_rand\": []}\n",
    "        self.eval_mode = False\n",
    "        self.swap_state = swap_state\n",
    "        self.log = log\n",
    "        self.wandb_name = wandb_name\n",
    "        self.wandb_run = None\n",
    "        self.do_optimize = do_optimize\n",
    "\n",
    "        if self.log and self.wandb_name is not None:\n",
    "            import wandb\n",
    "            self.wandb_run = wandb.init(project=\"ann-project\", name=wandb_name, reinit=True,\n",
    "                                        config={\"epsilon\": epsilon, \"gamma\": gamma, \"player\": player, \"memory_capacity\": memory_capacity,\n",
    "                                                \"target_update\": target_update, \"batch_size\": batch_size, \"learning_rate\": learning_rate,\n",
    "                                                \"log_every\": log_every, \"debug\": debug, \"swap_state\": swap_state, \"log\": log})\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Set the player to evaluation mode.\n",
    "        In this mode, the player will not explore and will only use the policy network to make decisions.\n",
    "        \"\"\"\n",
    "        self.eval_mode = True\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" Set the player to training mode. \"\"\"\n",
    "        self.eval_mode = False\n",
    "\n",
    "    def finish_run(self):\n",
    "        \"\"\"\n",
    "        Finish wandb run if present and in training mode.\n",
    "        \"\"\"\n",
    "        if not self.eval_mode and self.wandb_run:\n",
    "            self.wandb_run.finish()\n",
    "\n",
    "    def save_pretrained(self, save_path):\n",
    "        \"\"\" Save a pretrained model. \"\"\"\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "        config = dict(epsilon=None if callable(self.epsilon) else self.epsilon, gamma=self.gamma, player=self.player, memory_capacity=len(self.memory),\n",
    "                      target_update=self.target_update, learning_rate=self.learning_rate, batch_size=self.batch_size,\n",
    "                      log_every=self.log_every, debug=self.debug, avg_losses=self.avg_losses, avg_rewards=self.avg_rewards, m_values=self.m_values)\n",
    "        Path(save_path, \"config.json\").write_text(json.dumps(config))\n",
    "        torch.save(self.policy_net.state_dict(), Path(save_path, \"policy_net.pt\"))\n",
    "        torch.save(self.target_net.state_dict(), Path(save_path, \"target_net.pt\"))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, load_path):\n",
    "        \"\"\" Load a pretrained model. \"\"\"\n",
    "        config = json.loads(Path(load_path, \"config.json\").read_text())\n",
    "        policy_net = torch.load(Path(load_path, \"policy_net.pt\"))\n",
    "        target_net = torch.load(Path(load_path, \"target_net.pt\"))\n",
    "        player = cls(**config)\n",
    "        player.policy_net.load_state_dict(policy_net)\n",
    "        player.target_net.load_state_dict(target_net)\n",
    "        player.avg_losses = config[\"avg_losses\"]\n",
    "        player.avg_rewards = config[\"avg_rewards\"]\n",
    "        player.m_values = config[\"m_values\"]\n",
    "        return player\n",
    "\n",
    "    def set_player(self, player = 'X', j=-1):\n",
    "        self.player = player\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.last_reward = 0\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "\n",
    "    def empty(self, grid):\n",
    "        \"\"\" Return all empty positions in the grid. \"\"\"\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if grid[pos] == 0:\n",
    "                avail.append(i)\n",
    "        return avail\n",
    "\n",
    "    def random(self, grid):\n",
    "        \"\"\" Chose a random action from the available options. \"\"\"\n",
    "        avail = self.empty(grid)\n",
    "        return avail[random.randint(0, len(avail)-1)]\n",
    "\n",
    "    def opponent(self):\n",
    "        \"\"\" Return the opponent of the player. \"\"\"\n",
    "        return 'X' if self.player == 'O' else 'O'\n",
    "\n",
    "    def grid_to_state(self, grid):\n",
    "        \"\"\" Convert the grid to a state. \"\"\"\n",
    "        state = torch.zeros((3, 3, 2))\n",
    "\n",
    "        for i in range(len(grid)):\n",
    "            for j in range(len(grid[0])):\n",
    "                if VALUE_TO_PLAYER[grid[i, j]] == self.player:\n",
    "                    state[i, j] = torch.tensor([1, 0])\n",
    "                elif VALUE_TO_PLAYER[grid[i, j]] == self.opponent():\n",
    "                    state[i, j] = torch.tensor([0, 1])\n",
    "                else:\n",
    "                    state[i, j] = torch.tensor([0, 0])\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def maybe_swap_state(self, state):\n",
    "        \"\"\" Swap the state if necessary. \"\"\"\n",
    "        if self.swap_state:\n",
    "            return state.flip(dims=[2])\n",
    "        return state\n",
    "\n",
    "    def greedy(self, grid):\n",
    "        \"\"\" Return the best action according to the current policy network. \"\"\"\n",
    "        with torch.no_grad():\n",
    "            prediction = self.policy_net.predict(self.grid_to_state(grid).unsqueeze(0).to(DEVICE))\n",
    "            return prediction.item()\n",
    "\n",
    "    def decide(self, grid):\n",
    "        \"\"\" Decide on an action. \"\"\"\n",
    "        epsilon = self.epsilon(self.num_games) if callable(self.epsilon) else self.epsilon\n",
    "        if self.eval_mode or random.random() > epsilon:\n",
    "            return self.greedy(grid)\n",
    "        return self.random(grid)\n",
    "\n",
    "    def act(self, grid):\n",
    "        \"\"\" Act on the grid. \"\"\"\n",
    "        state = self.grid_to_state(grid)\n",
    "        action = self.decide(grid)\n",
    "\n",
    "        if not self.eval_mode:\n",
    "            if self.last_state is not None:\n",
    "                self.memory.push(self.maybe_swap_state(self.last_state), self.last_action, self.maybe_swap_state(state), self.last_reward)\n",
    "            \n",
    "            if self.do_optimize:\n",
    "                self.optimize()\n",
    "\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        self.last_reward = 0\n",
    "\n",
    "        return action\n",
    "\n",
    "    def end(self, grid, winner, invalid_move=False):\n",
    "        \"\"\" End callback of the game. \"\"\"\n",
    "        if not self.eval_mode:\n",
    "            self.num_games += 1\n",
    "            reward = 0\n",
    "\n",
    "            if winner == self.player:\n",
    "                reward = 1\n",
    "            elif winner == self.opponent() or invalid_move:\n",
    "                reward = -1\n",
    "\n",
    "            self.memory.push(self.maybe_swap_state(self.last_state), self.last_action, None, reward)\n",
    "\n",
    "            if self.do_optimize:\n",
    "                loss = self.optimize()\n",
    "\n",
    "            self.last_state = None\n",
    "            self.last_action = None\n",
    "\n",
    "            if self.log:\n",
    "                self.running_reward += reward\n",
    "\n",
    "                if loss is not None:\n",
    "                    self.running_loss += loss\n",
    "\n",
    "                if (self.num_games+1) % self.log_every == 0:\n",
    "                    avg_reward = self.running_reward / self.log_every\n",
    "                    self.avg_rewards.append(avg_reward)\n",
    "                    self.running_reward = 0\n",
    "\n",
    "                    avg_loss = self.running_loss / self.log_every\n",
    "                    self.avg_losses.append(avg_loss)\n",
    "                    self.running_loss = 0\n",
    "\n",
    "                    m_opt = calculate_m_opt(self)\n",
    "                    m_rand = calculate_m_rand(self)\n",
    "\n",
    "                    self.m_values[\"m_opt\"].append(m_opt)\n",
    "                    self.m_values[\"m_rand\"].append(m_rand)\n",
    "\n",
    "                    if self.wandb_name is not None:\n",
    "                        import wandb\n",
    "                        wandb.log({\"avg_reward\": avg_reward, \"avg_loss\": avg_loss, \"m_opt\": m_opt, \"m_rand\": m_rand})\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\" Optimize the policy network. \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), dtype=torch.bool).to(DEVICE)\n",
    "        non_final_next_states = [s for s in batch.next_state if s is not None]\n",
    "\n",
    "        if non_final_next_states:\n",
    "            non_final_next_states = torch.stack(non_final_next_states).to(DEVICE)\n",
    "        else:\n",
    "            non_final_next_states = None\n",
    "\n",
    "        state_batch = torch.stack(batch.state).to(DEVICE)\n",
    "        action_batch = torch.stack(batch.action).view(-1, 1).to(DEVICE)\n",
    "        reward_batch = torch.stack(batch.reward).view(-1, 1).to(DEVICE)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros((self.batch_size, 1)).to(DEVICE)\n",
    "\n",
    "        if non_final_next_states is not None:\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].view(-1, 1)\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.num_games % self.target_update == 0:\n",
    "            if self.debug:\n",
    "                print(f\"num_games={self.num_games}, loss={loss.item()}\")\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tic_env import OptimalPlayer\n",
    "\n",
    "epsilons = [0.001, 0.01, 0.1, 0.2]\n",
    "\n",
    "for eps in epsilons:\n",
    "    suboptimal_player = OptimalPlayer(epsilon=0.5)\n",
    "    q_player = DeepQPlayer(epsilon=eps)\n",
    "    play(suboptimal_player, q_player, episodes=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tic_env import OptimalPlayer\n",
    "\n",
    "epsilons = [0.001, 0.01, 0.1, 0.2]\n",
    "\n",
    "for eps in epsilons:\n",
    "    suboptimal_player = OptimalPlayer(epsilon=0.5)\n",
    "    q_player = DeepQPlayer(epsilon=eps, batch_size=1, memory_capacity=1)\n",
    "    play(suboptimal_player, q_player, episodes=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tic_env import OptimalPlayer\n",
    "\n",
    "EPS_MIN = 0.1\n",
    "EPS_MAX = 0.8\n",
    "n_stars = [1, 50, 100, 500, 1000, 5000, 10000, 20000, 40000]\n",
    "\n",
    "for n_star in n_stars:\n",
    "    suboptimal_player = OptimalPlayer(epsilon=0.5)\n",
    "    q_player = DeepQPlayer(epsilon=lambda n, n_star=n_star: max(EPS_MIN, EPS_MAX * (1 - n / n_star)))\n",
    "    play(suboptimal_player, q_player, episodes=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tic_env import OptimalPlayer\n",
    "\n",
    "EPS_MIN = 0.1\n",
    "EPS_MAX = 0.8\n",
    "N_STAR = 100\n",
    "epsilons = [0, 0.01, 0.1, 0.5, 1]\n",
    "\n",
    "for eps in epsilons:\n",
    "    other_player = OptimalPlayer(epsilon=eps)\n",
    "    q_player = DeepQPlayer(epsilon=lambda n: max(EPS_MIN, EPS_MAX * (1 - n / N_STAR)))\n",
    "    play(other_player, q_player, episodes=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0.001, 0.01, 0.1, 0.2, 0.3, 0.5]\n",
    "\n",
    "for eps in epsilons:\n",
    "    memory = ReplayMemory()\n",
    "    policy_net = DeepQNetwork()\n",
    "    target_net = DeepQNetwork()\n",
    "    q_player1 = DeepQPlayer(epsilon=eps, policy_net=policy_net, target_net=target_net, memory=memory)\n",
    "    q_player2 = DeepQPlayer(epsilon=eps, policy_net=policy_net, target_net=target_net, memory=memory, log=False, do_optimize=False)\n",
    "    play(q_player1, q_player2, episodes=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_MIN = 0.1\n",
    "EPS_MAX = 0.8\n",
    "n_stars = [1, 50, 100, 500, 1000, 5000, 10000, 20000, 40000]\n",
    "\n",
    "for n_star in n_stars:\n",
    "    memory = ReplayMemory()\n",
    "    policy_net = DeepQNetwork()\n",
    "    target_net = DeepQNetwork()\n",
    "    epsilon = lambda n, n_star=n_star: max(EPS_MIN, EPS_MAX * (1 - n / n_star))\n",
    "    q_player1 = DeepQPlayer(epsilon=epsilon, policy_net=policy_net, target_net=target_net, memory=memory)\n",
    "    q_player2 = DeepQPlayer(epsilon=epsilon, policy_net=policy_net, target_net=target_net, memory=memory, log=False, do_optimize=False)\n",
    "    play(q_player1, q_player2, episodes=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5770e5300d53e6f64d47d7e32e89bf51c6f2fdc3215612dcda7f4da521533ca6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('annproject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
